{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lozrigby/lab-encorder-models-bert/blob/main/lab_encorder_models_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-JnK7Zkhmj6"
      },
      "source": [
        "# Lab | Encorder Models - BERT\n",
        "\n",
        "---\n",
        "\n",
        "### Transformers' main components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2v9wkhQhuf4"
      },
      "source": [
        "**Hugging Face Transformers has two main components:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHkp89twf4Zq"
      },
      "source": [
        "\n",
        "1. The **tokenizer** prepares the text in a clean format, which the model understands.\n",
        "    - A token is a word or a sub-word unit. In BERT's vocabulary, the word \"good\" is one token and the word \"darwinism\" is two tokens  (\"darwin\" and \"ism\")\n",
        "    - The tokenizer transforms words into token-ids. With these token-ids, BERT can link words to any token it has already learned during pre-training.\n",
        "\n",
        "2. The **model** processes the tokenizer's ouput and returns a prediction, e.g. which class an input text belongs to.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4tmhG0piJ0P"
      },
      "source": [
        "Independently of the type of model (classification, summarisation, translation, etc.), these two components are almost the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNjndqEeVP5h",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#!pip install transformers~=4.31.0  # The Transformers library from Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0axlrhOhTGu"
      },
      "source": [
        "## Models like BERT (encoders)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hykl2GkBhq5F",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HneEleBYh_tY"
      },
      "outputs": [],
      "source": [
        "# load any classification model from the HuggingFace model hub\n",
        "# See here: https://huggingface.co/models?pipeline_tag=text-classification\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "# instantiate the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# instantiate the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zluWT2EpHd64"
      },
      "source": [
        "### Tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLKD6HULf4hX"
      },
      "outputs": [],
      "source": [
        "### 1. Tokenization\n",
        "# Tokenizer documentation: https://huggingface.co/transformers/main_classes/tokenizer.html\n",
        "\n",
        "text = 'I believe that the EU is trustworthy.'\n",
        "print(f\"Input text: '{text}'\\n\")\n",
        "\n",
        "input_ids = tokenizer(text, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
        "print(f\"\"\"The tokenizer splits the text string into separate tokens. A token is either an entire word,\n",
        "or a 'sub-word unit' in case of rare words (or punctuation).\n",
        "The word 'trustworthy', for example is split into two tokens: {tokenizer.tokenize(\"Trustworthy\")}.\n",
        "The main advantage of these sub-word units is that rare words cannot be out-of-vocabulary (an issue of other text-as-data approaches).\n",
        "Transformer models typically have a vocabulary of around 30.000 - 250.000 tokens, learned from the training data.\n",
        "Here is e.g. the vocabulary of DistilBERT: https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt\\n\"\"\")\n",
        "\n",
        "print(f\"The input text is split into the following tokens:\\n{tokenizer.tokenize(text)}.\")\n",
        "print(\"The tokenizer then maps each token to the corresponding token ID in the model's vocabulary:\")\n",
        "print(input_ids[0].tolist()[1:-1])\n",
        "print(\"Transformer models only understand these token IDs.\\n\")\n",
        "\n",
        "print(\"\"\"In addition, the tokenizer adds two special tokens:\n",
        " First, the [CLS] (classification) token is always added at the beginning.\n",
        "        While individual tokens represent individual (sub)words, the [CLS] token represents the entire text.\n",
        "        The [CLS] token \"is  used  as  the  aggregate sequence representation for classification tasks\" (Devlin et al. 2019: 4). Details: https://arxiv.org/pdf/1810.04805.pdf\n",
        " Second, the [SEP] token separates two texts. It is useful for tasks which require two text inputs, for example Questions & Answer tasks.\n",
        "        (It is not relevant in our case)\n",
        "\\n\"\"\")\n",
        "\n",
        "print(\"\"\"The final input for a BERT transformer model therefore looks like this:\"\"\")\n",
        "token_strings = tokenizer.convert_ids_to_tokens(ids=input_ids[0])\n",
        "#token_strings = tokenizer.tokenize(text)\n",
        "for token_id, token_string in zip(input_ids[0].tolist(), token_strings):\n",
        "  print(token_id, \" == \", token_string)\n",
        "\n",
        "\n",
        "# entire vocabulary: tokenizer.pretrained_vocab_files_map[\"vocab_file\"][\"distilbert-base-uncased\"]\n",
        "# https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw7qQhjGcZ8d"
      },
      "source": [
        "### Tokens (words) flowing through the neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1z8vNYtliIaJ"
      },
      "outputs": [],
      "source": [
        "### Processing the input with the model\n",
        "# Model class documentation: https://huggingface.co/transformers/main_classes/model.html\n",
        "# Documentation for DistilBERT specifically: https://huggingface.co/transformers/model_doc/distilbert.html\n",
        "\n",
        "print(f\"\"\"\\nAfter the preprocessing by the tokenizer, the model then feeds the sequence of tokens through the neural network.\n",
        "Each token is represented by a vector of 768 numbers (a 768 dimensional tensor).\n",
        "The tensor for the token \"trust\" looks for example like this before being fed into the first neural network layer\n",
        "(only 100 numbers are displayed):\\n\"\"\")\n",
        "print(model.distilbert.embeddings.word_embeddings(input_ids[0][7])[:100], \"\\n\")\n",
        "\n",
        "print(f\"\"\"The tensors for each token are then fed through and transformed by between 6-24~ neural network layers.\\n\"\"\")\n",
        "\n",
        "output = model(input_ids, output_hidden_states=True, output_attentions=False, return_dict=True)\n",
        "print(\"Same word after the first layer:\\n\\n\", output.hidden_states[1][0][7][:100], \"\\n\")  # same word embedding after the first attention layer\n",
        "print(\"Same word after the second layer:\\n\\n\", output.hidden_states[2][0][7][:100], \"\\n\")  # same word embedding after the second attention layer\n",
        "#print(\"Same word after the third layer:\\n\", output.hidden_states[3][0][7][:100], \"\\n\")  # same word embedding after the third attention layer\n",
        "print(\"\\n ... etc ...\\n\")\n",
        "\n",
        "print(f'The final output is a a contextualised representation of the sequence: \"{text}\"')\n",
        "#output.hidden_states[6][0][0][:100]  # final CLS token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ey7iZ13DsoKa"
      },
      "outputs": [],
      "source": [
        "print(\"This is what the different model layers ('the architecture') look like:\\n\")\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNThz6Mdc6Ei"
      },
      "source": [
        "### The final output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1ecRDjcsh-Z"
      },
      "outputs": [],
      "source": [
        "print(f\"\"\"At the end, Transformer models always output so called 'logits',\\n one number for each class the model was trained to classify text into.\\n\n",
        "Our input text was: '{text}'\\n\n",
        "These logis represent the predicted probability for our binary sentiment classification task:\\n\\n{output[\"logits\"][0].tolist()}\\n\"\"\")\n",
        "\n",
        "print(\"Logits are not very interpretable, so they are then converted to percentages.\\nEach percentages represents the model's prediction, which class the input text belongs to.\\n\")\n",
        "probabilities = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
        "label_names = model.config.id2label.values()\n",
        "prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(probabilities, label_names)}\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_AJJsLQHq3l"
      },
      "source": [
        "### Everything put together\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZK019KRAxHm"
      },
      "outputs": [],
      "source": [
        "## In short, the code looks like this:\n",
        "\n",
        "# load the relevant functions from HuggingFace and PyTorch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Choose any classification model from the model hub\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "# instantiate the tokenizer and the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# tokenization\n",
        "text = 'I believe that the EU is trustworthy.'\n",
        "input = tokenizer(text, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "# model prediction\n",
        "output = model(input, output_hidden_states=False, output_attentions=False, return_dict=True)\n",
        "probabilities = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
        "label_names = model.config.id2label.values()\n",
        "prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(probabilities, label_names)}\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrJNiO8HBX8v"
      },
      "outputs": [],
      "source": [
        "## Or via the simplified pipeline:\n",
        "from transformers import pipeline\n",
        "pipe_classification = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\", top_k=2)\n",
        "text = 'I believe that the EU is trustworthy.'\n",
        "pipe_classification(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCq3Nf6-z4LS"
      },
      "source": [
        "## Generative models like GPT (decoders)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4R3RIWJj0DBf"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# https://huggingface.co/gpt2\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "prompt = \"Today I believe we can finally\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_length=30)\n",
        "\n",
        "outputs_decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "print(outputs_decoded)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_FR2ctZ0yvG"
      },
      "outputs": [],
      "source": [
        "\n",
        "# https://huggingface.co/gpt2\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "prompt = \"Today I believe we can finally\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# gpt2's vocabulary: https://huggingface.co/gpt2/raw/main/vocab.json\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids, max_length=30,\n",
        "    output_scores=True, return_dict_in_generate=True,\n",
        "    output_attentions=False, do_sample=False\n",
        ")\n",
        "\n",
        "print(\"\\nThe output looks quite messy:\\n\")\n",
        "print(outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIcWIkzx3wmC"
      },
      "outputs": [],
      "source": [
        "print(\"GPT2's vocabulary is composed of 50257 tokens. Each has a 'word vector' composed of 768 numbers:\")\n",
        "print(model.transformer.wte)\n",
        "\n",
        "print(f\"\"\"\\nWe can look at GPT2's entire vocabulary here: https://huggingface.co/gpt2/raw/main/vocab.json\n",
        "\\nFor example, the token 'Love' is at position 18565.\n",
        "\\nWe can access it's word vector here (first 100 numbers):\\n\n",
        "{model.transformer.wte.weight[18565][:100]}\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dREWnBIZ5q0d"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(f\"\"\"\n",
        "While the outputs produce by classifiers like BERT are probabilities of classes,\n",
        "the outputs produced by generators like GPT2 are probabilities of tokens.\n",
        "\\nThese probabilities of tokens are in the 'outputs' object returned by model.generate()\n",
        "\\nThe IDs of the most probably tokens are:\n",
        "{outputs.sequences}\n",
        "\\nThese token IDs can be mapped to actuall words/tokens in the vocabulary:\n",
        "{tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)}\\n\\n\n",
        "\n",
        "Our original prompt was:\\n'{prompt}'\n",
        "GPT2 then tries to predict the most probable next token. One token after the other.\n",
        "\n",
        "To calculate the first token, it makes a prediction over ALL of the 50257 tokens it knows.\n",
        "Each of the 50257 tokens receives a probability.\n",
        "First the first token, the probability distribution over its ENTIRE vocabulary looks like this:\n",
        "{outputs.scores[0][0]}\n",
        "\n",
        "The ID of the most probable *first* token is {torch.argmax(outputs.scores[0][0], dim=0)}\n",
        "The corresponding token is: {tokenizer.decode(torch.argmax(outputs.scores[0][0], dim=0))}\n",
        "\n",
        "The ID of the most probable *second* token is {torch.argmax(outputs.scores[1][0], dim=0)}\n",
        "The corresponding token is: {tokenizer.decode(torch.argmax(outputs.scores[1][0], dim=0))}\n",
        "\n",
        "The ID of the most probable *third* token is {torch.argmax(outputs.scores[2][0], dim=0)}\n",
        "The corresponding token is: {tokenizer.decode(torch.argmax(outputs.scores[2][0], dim=0))}\n",
        "\n",
        "This is how GPT2 gradually generated the text:\n",
        "{tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)}\n",
        "\n",
        "The same principles apply to all generative LLMs like GPT4, Llama-2 etc.\n",
        "Only that they are bigger, with a better architecture and better fine-tuning.\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjTJh6DaMsSc"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLBWZ7tJ6K7b"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja80SRkiLP50"
      },
      "source": [
        "## Reflection  +  Q&A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8uhW4V_AU-m"
      },
      "source": [
        "\n",
        "**Reading, thinking & asking:** (5 min)\n",
        "* Write your answers to the following questions on a piece of paper / digital notebook. While thinking about these questions, also don't hesitate to ask any questions that come up in the chat/Slack.\n",
        "    * In your own words, write down the main differences between models like BERT and models like GPT with regard to their outputs.\n",
        "    * What could be disadvantages and advantages of these two different approaches (encoders vs. decoders)?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT is an encoder-based model that generates contextual embeddings for input tokens by considering both preceding and succeeding words. In contrast, GPT is a decoder-based model that generates text sequentially from left to right, predicting the next word based on previous words. Thus, BERT is optimized for understanding tasks, while GPT excels in text generation."
      ],
      "metadata": {
        "id": "Gltsv73UNe-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Disadvantages of bert/encoders**\n",
        "\n",
        "BERT’s bidirectional context provides a deep understanding of each word in relation to its full context, making it effective for tasks like sentiment analysis and question answering. It performs exceptionally well on tasks requiring comprehensive text understanding, such as named entity recognition and paraphrase detection.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yXHRh_e7NfyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**advantages of bert/encoders**\n",
        "\n",
        "BERT cannot generate text as it does not predict the next word in a sequence, limiting its use in creative text tasks. Also, as it is bidirectonal it makes inference slower because it must consider the entire sequence for each prediction."
      ],
      "metadata": {
        "id": "KXYG-XwROcjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Disadvantages of GPT/de-encoders**\n",
        "\n",
        "GPT is highly effective at generating coherent and contextually relevant text, making it ideal for applications like chatbots and content creation. Its sequential, left-to-right processing simplifies the text generation process, producing fluent continuations of prompts."
      ],
      "metadata": {
        "id": "0JXXtMxnOcS1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**advantages of GPT/de-encoders**\n",
        "\n",
        "GPT's left-to-right processing limits its ability to fully understand a word’s context using future tokens, affecting tasks requiring bidirectional context. It is also constrained by its context window size, limiting the number of previous tokens it can consider when generating text, which can be problematic for understanding or generating lengthy texts."
      ],
      "metadata": {
        "id": "ABD_pm8iOlvO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EKru1UrZOvZF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}